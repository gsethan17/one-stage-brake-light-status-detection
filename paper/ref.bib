

@article{ye2022adaptive,
  title={An adaptive attention fusion mechanism convolutional network for object detection in remote sensing images},
  author={Ye, Yuanxin and Ren, Xiaoyue and Zhu, Bai and Tang, Tengfeng and Tan, Xin and Gui, Yang and Yao, Qin},
  journal={Remote Sensing},
  volume={14},
  number={3},
  pages={516},
  year={2022},
  publisher={MDPI}
}

@article{zeng2022small,
  title={A small-sized object detection oriented multi-scale feature fusion approach with application to defect detection},
  author={Zeng, Nianyin and Wu, Peishu and Wang, Zidong and Li, Han and Liu, Weibo and Liu, Xiaohui},
  journal={IEEE Transactions on Instrumentation and Measurement},
  volume={71},
  pages={1--14},
  year={2022},
  publisher={IEEE}
}

@article{assunccao2022real,
  title={Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism},
  author={Assun{\c{c}}{\~a}o, Eduardo and Gaspar, Pedro D and Mesquita, Ricardo and Sim{\~o}es, Maria P and Alibabaei, Khadijeh and Veiros, Andr{\'e} and Proen{\c{c}}a, Hugo},
  journal={Remote Sensing},
  volume={14},
  number={17},
  pages={4217},
  year={2022},
  publisher={MDPI}
}

@article{li2020generalized,
  title={Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection},
  author={Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21002--21012},
  year={2020}
}

@inproceedings{zheng2020distance,
  title={Distance-IoU loss: Faster and better learning for bounding box regression},
  author={Zheng, Zhaohui and Wang, Ping and Liu, Wei and Li, Jinze and Ye, Rongguang and Ren, Dongwei},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={12993--13000},
  year={2020}
}

%% two-stage
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}

@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

%% yolo
@article{terven2023comprehensive,
  title={A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond},
  author={Terven, Juan and Cordova-Esparza, Diana},
  journal={arXiv preprint arXiv:2304.00501},
  year={2023}
}


@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}
@inproceedings{redmon2017yolo9000,
  title={YOLO9000: better, faster, stronger},
  author={Redmon, Joseph and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7263--7271},
  year={2017}
}
@article{redmon2018yolov3,
  title={Yolov3: An incremental improvement},
  author={Redmon, Joseph and Farhadi, Ali},
  journal={arXiv preprint arXiv:1804.02767},
  year={2018}
}
@article{bochkovskiy2020yolov4,
  title={Yolov4: Optimal speed and accuracy of object detection},
  author={Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2004.10934},
  year={2020}
}
@article{li2022yolov6,
  title={YOLOv6: A single-stage object detection framework for industrial applications},
  author={Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and others},
  journal={arXiv preprint arXiv:2209.02976},
  year={2022}
}
@inproceedings{wang2023yolov7,
  title={YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7464--7475},
  year={2023}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

o2010rear, kim2010effective, gao2019line, parvin2021vision
%%%%%%% vehicle detection through tail lamp detection %%%%%%%%%%%%
@article{parvin2021vision,
  title={Vision-based on-road nighttime vehicle detection and tracking using taillight and headlight features},
  author={Parvin, Shahnaj and Rozario, Liton Jude and Islam, Md Ezharul and others},
  journal={Journal of Computer and Communications},
  volume={9},
  number={03},
  pages={29},
  year={2021},
  publisher={Scientific Research Publishing}
}

@article{shao2020feature,
  title={Feature enhancement based on CycleGAN for nighttime vehicle detection},
  author={Shao, Xiaotao and Wei, Caike and Shen, Yan and Wang, Zhongli},
  journal={IEEE Access},
  volume={9},
  pages={849--859},
  year={2020},
  publisher={IEEE}
}

@article{gao2019line,
  title={On-line vehicle detection at nighttime-based tail-light pairing with saliency detection in the multi-lane intersection},
  author={Gao, Fei and Ge, Yisu and Lu, Shufang and Zhang, Yuanming},
  journal={IET Intelligent Transport Systems},
  volume={13},
  number={3},
  pages={515--522},
  year={2019},
  publisher={Wiley Online Library}
}

@article{kim2010effective,
  title={An effective method of head lamp and tail lamp recognition for night time vehicle detection},
  author={Kim, Hyun-Koo and Kuk, Sagong and Kim, MK and Jung, Ho-Youl},
  journal={World Academy of Science, Engineering and Technology},
  volume={44},
  pages={1091--1094},
  year={2010},
  publisher={Citeseer}
}

@article{o2010rear,
  title={Rear-lamp vehicle detection and tracking in low-exposure color video for night conditions},
  author={O'Malley, Ronan and Jones, Edward and Glavin, Martin},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={11},
  number={2},
  pages={453--462},
  year={2010},
  publisher={IEEE}
}

%%%%%%% lamp detection for ADAS %%%%%%%%%%%%
A Highly Efficient Vehicle Taillight Detection Approach Based on Deep Learning
yolov3 tiny model with SSP but, there is no inference speed 
proposed a method to detect whether vehicle lights are turned on using the YOLO v3-tint model. The proposed method uses a spatial pyramid pooling module to detect whether vehicle lights are turned on despite various shapes of vehicles to improve detection accuracy during neural network learning.
@article{li2020highly,
  title={A highly efficient vehicle taillight detection approach based on deep learning},
  author={Li, Qiaohong and Garg, Sahil and Nie, Jiangtian and Li, Xiang and Liu, Ryan Wen and Cao, Zhiguang and Hossain, M Shamim},
  journal={IEEE transactions on intelligent transportation systems},
  volume={22},
  number={7},
  pages={4716--4726},
  year={2020},
  publisher={IEEE}
}
A vision-based hierarchical framework for autonomous front-vehicle taillights detection and signal recognition
HSV = hue saturation value filter, SVM = Support Vector Machine
HSV, HOG, SVM
only day
studied tail light detection and signal recognition including turn signals. Their approach included bounding box extraction, a preprocessing stage in HSV colorspace, and signal classification. Bounding box extraction was based on HOG, a pairing algorithm, as well as a support vector machine. The preprocessing stage included HSV colorspace, clustering, and the orientation and location of taillight pairs inside the bounding box. A dictionary learning algorithm was used for the classification stage. The study primarily focused on closely located vehicles, i.e., 5â€“40 m.
In machine vision-based vehicle brake light lighting detection research [13], research is being conducted based on the number of red color pixels of vehicle brake lights using RGB color information.
@inproceedings{cui2015vision,
  title={A vision-based hierarchical framework for autonomous front-vehicle taillights detection and signal recognition},
  author={Cui, Zhiyong and Yang, Shao-Wen and Tsai, Hsin-Mu},
  booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems},
  pages={931--937},
  year={2015},
  organization={IEEE}
}
@article{he2015spatial,
  title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={9},
  pages={1904--1916},
  year={2015},
  publisher={IEEE}
}


Appearance-based brake-lights recognition using deep learning and vehicle detection
HOG, CNN
Day
have proposed another brake light detection algorithm based on extracting vehicle rears utilizing a HOG detector. They modified the original HOG detector to better suit the purpose of using road- and vehicle-specific information. The brake light status of a vehicle was evaluated using a CNN classifier, which was trained with images of vehicles with brake lights either on or off. Their classifier achieved an average accuracy of 89% with ten-fold cross-validation on their self-gathered non-public data.
a feature was extracted and learned from labeled vehicle regions using a pre-trained AlexNet model to detect vehicles. The vehicle is detected using the trained AlexNet object detector, and the location is determined using the brake lights pattern in the detected vehicle region. In their method, it is necessary to generate brake light patterns in advance from a large database of vehicle rear images. However, there is a problem that the patterns of brake lights differ depending on the size, type, and shape of the vehicle, and the patterns may vary depending on the type of vehicle in the vehicle database built in advance.
@inproceedings{wang2016appearance,
  title={Appearance-based brake-lights recognition using deep learning and vehicle detection},
  author={Wang, Jian-Gang and Zhou, Lubing and Pan, Yu and Lee, Serin and Song, Zhiwei and Han, Boon Siew and Saputra, Vincensius Billy},
  booktitle={2016 IEEE intelligent vehicles symposium (IV)},
  pages={815--820},
  year={2016},
  organization={IEEE}
}

Daytime preceding vehicle brake light detection using monocular vision
HOG, LAB
HOG = histogram of oriented gradients, LAB = L*a*b filter
Day
utilized HOG to capture the vehicle bounding boxes. They identified lower two tail light candidate regions from these bounding boxes as radially symmetric areas that were found on a common horizontal plane. In other words, the two brake lights should exist as a pair symmetric with respect to the vehicle center line. These regions were filtered with an L*a*b filter. The brake light status was then detected based on high red chromaticity difference to the tail light region, as the lights only occupy some of the tail light region. Their approach yielded an accuracy of 87.6% on their own non-public dataset.
@article{chen2015daytime,
  title={Daytime preceding vehicle brake light detection using monocular vision},
  author={Chen, Hua-Tsung and Wu, Yi-Chien and Hsu, Chun-Chieh},
  journal={IEEE Sensors Journal},
  volume={16},
  number={1},
  pages={120--131},
  year={2015},
  publisher={IEEE}
}
Frequency-tuned taillight-based nighttime vehicle braking warning system
NDM = Nakagami-distribution model
Night
used a light distribution based Nakagami-distribution model and achieved an average detection accuracy of 76%, with extensive testing in urban and highway routes, as well as under different weather conditions.
@article{chen2012frequency,
  title={Frequency-tuned taillight-based nighttime vehicle braking warning system},
  author={Chen, Duan-Yu and Peng, Yang-Jie},
  journal={IEEE Sensors Journal},
  volume={12},
  number={11},
  pages={3285--3292},
  year={2012},
  publisher={IEEE}
}
Predictive brake warning at night using taillight characteristic
RGB filter
Night
developed a system that detected brake lights in the darkness with a simplistic RGB model. They used a test sample of 45 images and managed to classify brake light status with 87% accuracy.
@inproceedings{thammakaroon2009predictive,
  title={Predictive brake warning at night using taillight characteristic},
  author={Thammakaroon, Peachanika and Tangamchit, Poj},
  booktitle={2009 IEEE International Symposium on Industrial Electronics},
  pages={217--221},
  year={2009},
  organization={IEEE}
}a

Vision-based method for forward vehicle brake lights recognition
proposed detecting forward-driving vehicles to prevent vehicle rear collision warnings and detecting whether RGB color information-based vehicle braking was operated. The proposed method uses a Haar-based AdaBoost cascade classifier to detect vehicles. Then, the position of the vehicle brake lights is detected using the color, shape, and structural characteristic information of the detected vehicle between image frames. Moreover, it is detected whether the change of the RGB color value at the vehicle brake lights position is turned on using the threshold method. Their proposal is a method applicable to daytime hours and has limitations in application in places where various lighting and environments change, such as road tunnels.
@article{liu2015vision,
  title={Vision-based method for forward vehicle brake lights recognition},
  author={Liu, Wei and Bao, Hong and Zhang, Jun and Xu, Cheng},
  journal={International Journal of Signal Processing, Image Processing and Pattern Recognition},
  volume={8},
  number={6},
  pages={167--180},
  year={2015}
}
@inproceedings{freund1996experiments,
  title={Experiments with a new boosting algorithm},
  author={Freund, Yoav and Schapire, Robert E and others},
  booktitle={icml},
  volume={96},
  pages={148--156},
  year={1996},
  organization={Citeseer}
}




A Collision Warning Oriented Brake Lights Detection and Classification Algorithm Based on a Mono Camera Sensor
the vehicle is first detected using the YOLO object detector. Moreover, after converting the color space of the detected vehicle region into the L*a*b color space, a method using the support vector machine (SVM) learner was proposed. Their proposed method is a method to detect whether vehicle brake lights are turned on general roads during the daytime, and a detection accuracy of about 96.3% was presented.
@inproceedings{nava2019collision,
  title={A collision warning oriented brake lights detection and classification algorithm based on a mono camera sensor},
  author={Nava, Dario and Panzani, Giulio and Savaresi, Sergio M},
  booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  pages={319--324},
  year={2019},
  organization={IEEE}
}


used the you only look once version 3 (YOLO v3) vehicle detectors trained on 100,000 COCO datasets to detect vehicles in input images. Then, in order to detect whether the brake lights of the detected vehicle are turned on or not, the RGB color space of the detected vehicle region is converted into a L*a*b color space and then learned using a random forest algorithm. The proposal assumes that there are brake lights at the center of the vehicle and that there are no brake light failures. High-definition input images are required by performing L*a*b color space filtering to perform learning on images that are turned on and off. In addition, when the vehicle taillights are automatically turned on in a low-illuminance environment such as in a road tunnel, there is a problem in that the brake lights may be mis-detected as being turned on. Additionally, the random forest algorithm has a disadvantage in that it consumes a lot of memory during training.
@article{pirhonen2022brake,
  title={Brake light detection algorithm for predictive braking},
  author={Pirhonen, Jesse and Ojala, Risto and Kivek{\"a}s, Klaus and Veps{\"a}l{\"a}inen, Jari and Tammi, Kari},
  journal={Applied Sciences},
  volume={12},
  number={6},
  pages={2804},
  year={2022},
  publisher={MDPI}
}
@article{pirhonen2022predictive,
  title={Predictive Braking With Brake Light Detectionâ€”Field Test},
  author={Pirhonen, Jesse and Ojala, Risto and Kivek{\"a}s, Klaus and Tammi, Kari},
  journal={IEEE Access},
  volume={10},
  pages={49771--49780},
  year={2022},
  publisher={IEEE}
}

tennels
@article{kim2022detecting,
  title={Detecting the Turn on of Vehicle Brake Lights to Prevent Collisions in Highway Tunnels},
  author={Kim, JongBae},
  journal={Sustainability},
  volume={14},
  number={21},
  pages={14322},
  year={2022},
  publisher={MDPI}
}









@article{reason1978motion,
  title={Motion sickness adaptation: a neural mismatch model},
  author={Reason, James T},
  journal={Journal of the Royal Society of Medicine},
  volume={71},
  number={11},
  pages={819--829},
  year={1978},
  publisher={SAGE Publications Sage UK: London, England}
}

@book{reason1975motion,
  title={Motion sickness.},
  author={Reason, James T and Brand, Joseph John},
  year={1975},
  publisher={Academic press}
}

@article{sae20213016,
  title={3016â€”Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems},
  author={SAE, J},
  journal={SAE International, On-Road Automated Driving (ORAD) Committee: Warrendale, PA, USA},
  year={2021}
}

@article{iskander2019car,
  title={From car sickness to autonomous car sickness: A review},
  author={Iskander, Julie and Attia, Mohammed and Saleh, Khaled and Nahavandi, Darius and Abobakr, Ahmed and Mohamed, Shady and Asadi, Houshyar and Khosravi, Abbas and Lim, Chee Peng and Hossny, Mohammed},
  journal={Transportation research part F: traffic psychology and behaviour},
  volume={62},
  pages={716--726},
  year={2019},
  publisher={Elsevier}
}

@article{diels2016self,
  title={Self-driving carsickness},
  author={Diels, Cyriel and Bos, Jelte E},
  journal={Applied ergonomics},
  volume={53},
  pages={374--382},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{zou2023object,
  title={Object detection in 20 years: A survey},
  author={Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  journal={Proceedings of the IEEE},
  volume={111},
  pages={257--276},
  year={2023},
  publisher={IEEE}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@misc{ brake-light-detection_dataset,
    title = { Brake-Light-Detection Dataset },
    type = { Open Source Dataset },
    howpublished = { Available online: \url{ https://universe.roboflow.com/imlab-kookmin-univ/brake-light-detection } (accessed on 21 June 2023)},
    journal = { Roboflow Universe },
    publisher = { Roboflow },
}

@misc{coco_benchmark,
  title={COCO Detection Leaderboard},
  howpublished={Available online: \url{https://cocodataset.org/#detection-leaderboard} (accessed on 21 June 2023)}
}

@misc{roboflow,
  title={Roboflow},
  howpublished={Available online: \url{https://roboflow.com} (accessed on 21 June 2023)},
}

@misc{YOLOv5,
  title={Ultralytics YOLOv5},
  howpublished={Available online: \url{https://docs.ultralytics.com/yolov5/} (accessed on 21 June 2023)},
}

@misc{YOLOv8,
  title={Ultralytics YOLOv8},
  howpublished={Available online: \url{https://docs.ultralytics.com/} (accessed on 21 June 2023)},
}

@misc{Label_Studio,
  title={{Label Studio}: Data labeling software},
  howpublished={Available online: \url{https://github.com/heartexlabs/label-studio} (accessed on 21 June 2023)},
}

%%%%%%%%% methodology %%%%%%%%%%%

% DRER
@article{oh2021drer,
  title={Drer: Deep learning--based driverâ€™s real emotion recognizer},
  author={Oh, Geesung and Ryu, Junghwan and Jeong, Euiseok and Yang, Ji Hyun and Hwang, Sungwook and Lee, Sangho and Lim, Sejoon},
  journal={Sensors},
  volume={21},
  number={6},
  pages={2166},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

% front image
% stress
@inproceedings{gao2014detecting,
  title={Detecting emotional stress from facial expressions for driving safety},
  author={Gao, Hua and Y{\"u}ce, Anil and Thiran, Jean-Philippe},
  booktitle={2014 IEEE International Conference on Image Processing (ICIP)},
  pages={5961--5965},
  year={2014},
  organization={IEEE}
}
