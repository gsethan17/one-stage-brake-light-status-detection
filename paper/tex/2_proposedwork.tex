\section{Proposed Work}
\label{sec:proposed}

In this study, we propose one-stage brake light detection network based on YOLOv8. Proposed network detects not only the driving vehicle but also the vehicle's brake light status in a one-stage. 
The input of the network is a single forward image taken from on ego-vehicle and outputs are the brake light status categories along with the 2D bounding boxes for all driving vehicle in the input image as shown in Figure \ref{fig:workflow}.
2D bounding box is represented by four numerical information $x$, $y$, $w$, and $h$. $x$ and $y$ is 2D center coordinates of the bounding box and $w$ and $h$ is width and height of the bounding box, respectively. 
The number of classes $m$ is defined as $2$, which means the brake light status categories and classes score $C$ means the probability value of each category as follows:

\begin{equation}
    m=2, \;\;\; C   \begin{cases}
        C_{1}=1.0 & \text{ If the brake light is turned off } \\
        C_{2}=1.0 & \text{ If the brake light is turned on }
        \end{cases}
        , \quad w.r.t \sum_{i=1}^{m}{C_{i}}=1.0
\end{equation}

where $C_{1}$ is the probability that the brake light is turned off, and $C_{2}$ is the probability that the brake light is turned on of the detected vehicle.
Therefore, proposed network outputs the bounding boxes for all vehicles present in the input image regardless of the vehicle type, along with the probability values for the two defined classes.
Here, all vehicles encompass not only conventional passenger cars but also motorcycles, buses, trucks and special vehicles.

\subsection{Dataset}
\label{sec:method_dataset}
To perform a specific task of detecting brake light status with vehicle, we need to prepare a custom dataset.
Custom dataset preparation is divided into input image collection and annotation process.
To collect forward images of the driving vehicle, we use a dashboard camera, which is designed to capture video footage of the road ahead while driving. Two different cameras (FINEVu GX2000 and Mercedes-Benz dashcam) are used, each is mounted on the windshield inside of different vehicle.
After driving in the vehicle mounted with the camera, images are collected by extracting single images from the video stored in the dashboard camera. 
However, since the dashboard camera typically captures more than 30 images per second, collecting every single image as an input image will adversely affect learning due to many similar images.
To circumvent this problem, we collect the input images at $T$ second intervals from the video.

Another thing that should not be overlooked while using the dashboard camera images is the camera's image post-processing capabilities.
The dashboard camera include various image post-processing features such as brightness correction and High Dynamic Range (HDR), as it is essential to capture clear details in critical situations such as car accidents.
Therefore, it is necessary to consider the pre-processing of input data in order to train a network that exhibits reliable performance even with a general camera, which has no image post-processing features. The data pre-processing we consider is described in Section \ref{sec:exp_pre}.

\begin{figure}[t]%

    \subfloat[Light reflection from LED lights]{{\includegraphics[height=3.85cm]{fig/003202.jpg} }}%
    \subfloat[Confusion with tail lights]{{\includegraphics[height=3.85cm]{fig/000117.jpg} }}%
    \hfill
    \subfloat[Preceding image of (a)]{{\includegraphics[height=3.85cm]{fig/003201.jpg} }}%
    \subfloat[Preceding image of (b)]{{\includegraphics[height=3.85cm]{fig/000116.jpg} }}%

\caption{Illustrative cases in which it is difficult to determine the operation of brake lights based on single image} 
% (a) Light reflection from LED lights. (b) Confusion with tail lights. (c) Preceding image of light reflection case. (d) Preceding image of confusion with tail light case}
\label{fig:label_hard}%
\end{figure}

To perform supervised transfer learning, annotation information in the same form as output of proposed network is required.
Six annotation experts manually label information such as the bounding box ($x, y, w, h$) and brake light status ($C$) of vehicles appearing in each collected image.
The experts annotate using LabelImg, which is the popular open source image annotation tool.
This tool is open-source software and is now has become part of the open-source data labeling tool, Label Studio \cite{Label_Studio}, which offers more flexible functions.
There are several factors that make it difficult for trained experts to annotate from single image, even with useful tool.
Representatively, there are light reflection from Light Emitting Diode (LED) lights and confusion with tail lights.
Most vehicle brake lights are composed of LED lights and LED lights easily reflect ambient light.
When ambient light is reflected from an LED light and reaches the camera, the LED itself may appear to emit light even through the LED is not turned on.
This leads to the same illusion as if the brake light of the vehicle is turned on even though it is not turned on.
Looking at Figure \ref{fig:label_hard}-(a), it is not easy to determine whether the brake light of the vehicle in the center of the image is turned on due to the bright light around it. 
Even when comparing the light intensity of the vehicle on the left side of the image where the brake light actually is turned on and the vehicle on the right side of the image where the brake light actually is turned off, it shows an intermediate level.
While the light reflection issue mainly occurs during the day time when there is a lot of ambient light, it occurs confusion issue with tail lights at night.
Looking at Figure \ref{fig:label_hard}-(b), it is not easy to determine whether the vehicle in the center of the image has only the tail light turned on or the brake light is turned on.
If there are no surrounding vehicles in the image, this confusion will be even greater.
In order to overcome this difficulty, the experts annotate by referring to several preceding or succeeding images. Figure \ref{fig:label_hard}-(c) and (d) are preceding images within a short span of Figure \ref{fig:label_hard}-(a) and (b), respectively.
When referring to the preceding images, it is very easy to determine whether the brake lights of the vehicles in the center of \ref{fig:label_hard}-(a) and (b) are turned on.




\subsection{YOLOv8}
YOLO, which stands for "You Only Look Once", is one of the popular multi-object detection algorithms \cite{redmon2016you}.
As the name suggests, YOLO aims to provide detection results by analyzing the image only once. 
Before YOLO was proposed, many multi-object detection algorithms adopted multiple stages to accurately detect object location and class \cite{girshick2014rich, he2015spatial, girshick2015fast, ren2015faster}.
However, these approaches had limitations in real-time applications due to the need for multiple steps.
YOLO introduced the idea of simultaneously detecting the locations and classes of objects.
YOLO's key innovation is that a single neural network performs object detection.
From the time it was first proposed, YOLO has been known for its fast inference speed and high accuracy compared to other object detection algorithms, and has gradually evolved from YOLOv1 to YOLOv8 \cite{redmon2016you, redmon2017yolo9000, redmon2018yolov3, bochkovskiy2020yolov4, YOLOv5, li2022yolov6, wang2023yolov7, YOLOv8}.
Therefore, our proposed network is based on the YOLOv8, which is latest state-of-the-art one-stage multi-object detection algorithm.

YOLOv8 \cite{YOLOv8} was officially released in January 2023 and underwent a update based on YOLOv5 \cite{YOLOv5}.
The notable updates in YOLOv8 include a structural change in the partial bottleneck, a shift to an anchor-free approach with the decoupled head, and a change in the activation function of the top layer \cite{terven2023comprehensive}.
The loss functions used in YOLOv8 include binary cross-entropy for classification loss and a complete intersection over union (CIoU) \cite{zheng2020distance} and a distribution focal loss (DFL) \cite{li2020generalized} loss functions for localization loss.
The output of propose network consists of $8,400$ bounding boxes with $6$ parameters per input image.
These $6$ parameters represent the 2D center coordinates, width, and height of the bounding box ($x, y, w, h$), and the probability values for each class, brake light is turned off or on. 
Among the $8,400$ output bounding boxes, insignificant detections are filtered out through post-processing as non-maximum suppression (NMS).
This helps eliminate redundant and overlapping bounding boxes, resulting in a more refined and accurate set of detections.


% \begin{equation}
    % loss_{iou} = \frac{{\sum ((1.0 - iou) \cdot weight)}}{targetscoressum}
% \end{equation}
% used pretrained weights trained by COCO and image Net detaset

% bbox loss = iou
% dfl loss
% cls loss