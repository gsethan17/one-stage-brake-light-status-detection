\section{Experiments}
\label{sec:experiments}

The experimental evaluation of the proposed method is presented in this section. 
Section \ref{sec:exp_pre} provides details about the custom dataset collected as described in Section \ref{sec:proposed}, as well as preprocessing steps.
Section \ref{sec:exp_transfer} focuses on the transfer learning process of YOLOv8 using the custom dataset.
The experimental results are presented in detail in Section \ref{sec:exp_results}.

\subsection{Custom dataset \& Preprocessing}
\label{sec:exp_pre}
A large-scale custom dataset consisting over 16 hours of data was collected. 
Videos recorded from dashboard cameras during real road driving were utilized, resulting in a total of more than $11,000$ images.
These images were obtained by setting the time interval $T$ to $5$ seconds.
Manual annotation was performed on all images to label the bounding box of the driving vehicle and the brake light status of each vehicle.
The total number of annotations exceeded $30,000$.
To ensure a balanced and diverse dataset, various driving scenarios were included, such as daytime, nighttime, city, highway, and tunnel environments.
Precautions were taken to avoid bias toward any specific category, ensuring information about the number of images and annotations in the train and test sets for each category of brake light status.
The details of numbers of images and annotations in the train and test sets in each category of brake light status are given in Table \ref{tab:dataset}.

\begin{table}[h]
    \caption{Details of train and test datasets}
    \label{tab:dataset}
    % \resizebox{\textwidth*0.5}{!}{%
    \begin{tabular}{p{5cm} p{5cm} p{5cm}}
        % {lrr}
    \toprule
    \multicolumn{1}{c}{Number of}                          & \multicolumn{1}{c}{Train} & \multicolumn{1}{c}{Test} \\
    \midrule
    Images                              & \multicolumn{1}{r}{7,892}                     & \multicolumn{1}{r}{3,196}                    \\
    Annotations                         & \multicolumn{1}{r}{19,913}                    & \multicolumn{1}{r}{10,531}                   \\
    \multicolumn{1}{c}{Brake Light Off ($C_{1}=1.0$)} & \multicolumn{1}{r}{10,851}                    & \multicolumn{1}{r}{5,999}                    \\
    \multicolumn{1}{c}{Brake Light On ($C_{2}=1.0$)}  & \multicolumn{1}{r}{9,062}                     & \multicolumn{1}{r}{4,532}                   \\
    \bottomrule
    \end{tabular}%
    % }
\end{table}





To make our custom dataset trainable with YOLOv8 and achieve robust performance, several preprocessing steps are necessary.
The first crucial processes involve image resizing and normalization.
In order to maintain a consistent input size, the width and height of all images were resized to $I_{w}$ and $I_{h}$, respectively. 
In this study, both $I_{w}$ and $I_{h}$ were defined as $640$.
Regarding image normalization, min-max normalization was applied to normalize the pixel values.
The normalization precess ensure that all pixel values fall within a specific range, typically between $0$ and $1$, and is performed as follows:

\begin{figure}[b!]%

    \subfloat[Reshape]{{\includegraphics[height=3.4cm]{fig/preprocessing_reshape.jpeg} }}%
    \subfloat[horizontal flip]{{\includegraphics[height=3.4cm]{fig/preprocessing_flip.jpeg} }}%
    \subfloat[Crop]{{\includegraphics[height=3.4cm]{fig/preprocessing_crop.jpeg} }}%
    \subfloat[Black-box cutout]{{\includegraphics[height=3.4cm]{fig/preprocessing_cutout.jpeg} }}%
    \hfill
    \subfloat[Brightness increase]{{\includegraphics[height=3.4cm]{fig/preprocessing_brightness_max.jpeg} }}%
    \subfloat[Brightness decrease]{{\includegraphics[height=3.4cm]{fig/preprocessing_brightness_min.jpeg} }}%
    \subfloat[Blur]{{\includegraphics[height=3.4cm]{fig/preprocessing_blur} }}%
    \subfloat[Noise]{{\includegraphics[height=3.4cm]{fig/preprocessing_noise.jpeg} }}%

\caption{Examples of individual random modifications applied during the preprocessing stage. (c) to (n) represent example images that reflect the maximum extent of each random modification considered.}
\label{fig:random}%
\end{figure}

\begin{equation}
    x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}
\end{equation}
where $x$ and $x_{norm}$ are origin and normalized pixel value, respectively and $x_{max}$ and $x_{min}$ are the maximum and minimum pixel value of the image, respectively.
In this study, the values for $x_{max}$ and $x_{min}$ were set to $255.0$ and $0.0$, respectively, following the usual convention.
Furthermore, various data augmentation techniques were applied to enhance the robustness of the inference performance.
Random horizontal flipping and image cropping were performed to generate variations of the collected images that could realistically occur.
To improve detection performance in cases of occlusion, random black-box cutout augmentation was also applied.
Finally, to ensure robustness across different camera setups, the quality of the input images was intentionally degraded using various methods.
As stated in Section \ref{sec:method_dataset}, the dashboard cameras used for image acquisition are equipped with various postprocessing methods to captured high-quality images.
To ensure the robust performance of the trained network even with general cameras, random modifications such as brightness changes, blur, and noise injection were applied to the images.
The details of all random modifications applied during the preprocessing stage are as follows:

\begin{figure}[b!]%

    \subfloat{{\includegraphics[height=4.5cm]{fig/image20.png} }}%
    \subfloat{{\includegraphics[height=4.5cm]{fig/image21.png} }}%
    \subfloat{{\includegraphics[height=4.5cm]{fig/image22.png} }}%
    \hfill
    \subfloat{{\includegraphics[height=4.5cm]{fig/image23.png} }}%
    \subfloat{{\includegraphics[height=4.5cm]{fig/image24.png} }}%
    \subfloat{{\includegraphics[height=4.5cm]{fig/image25.png} }}%

\caption{Preprocessed input images of custom dataset}
\label{fig:custom_dataset}%
\end{figure}

\begin{itemize}
    \item{Crop: zoom rate chosen from uniform distribution within the range of $0\%$ to $20\%$}
    \item{Cutout: a maximum size of black-box is $20\%$ of image size, a maximum of $3$ boxes}
    \item{Brightness: adjustment with a range of minimum $-25\%$ to maximum $+25\%$}
    \item{Blur: a gaussian blur with a maximum kernel size of $3 \times 3$}
    \item{Noise: add noise to a maximum of $5\%$ of pixels}
\end{itemize}
The images illustrating each modification can be found in Figure \ref{fig:random}.
Figure \ref{fig:custom_dataset} provides examples of preprocessed images.
All preprocessing steps were performed using Roboflow \cite{roboflow}, a comprehensive platform for computer vision and image processing tasks.
The preprocessed train dataset is publicly available for access \cite{brake-light-detection_dataset}.




\subsection{Transfer learning}
\label{sec:exp_transfer}
In this study, we conducted transfer learning on all models provided by YOLOv8 \cite{YOLOv8} to develop a detection network that achieves real-time inference in a driving vehicle while maintaining accurate detection.
The YOLOv8 architecture offers five different models of varying sizes, ranging from the smallest model (YOLOv8n) to the largest model (YOLOv8x), as shown in Table \ref{tab:yolov8}.
To ensure consistency, the same set of hyperparameters was applied to all models during training.
A random 20\% of the train dataset was set aside as the validation dataset for monitoring the training progress.
The initial parameter values for each model were obtained from the pretrained parameters officially provided by YOLOv8.
The training process consisted fo $300$ iterations, with a patience value of $20$.
If there was no observable improvement in the validation loss over the most recent $20$ iterations, the training was terminated early to save time and resources.
Stochastic gradient descent (SGD) optimizer with a learning rate of $0.01$ was used, incorporating both momentum and Nesterov Accelerated Gradient (NAG) techniques \cite{sutskever2013importance} with a momentum coefficient of $0.937$.
The training loss was calculated using three loss function: binary cross-entropy, CIoU, and DFL.
The weighting factors assigned th these loss functions were $0.5$, $7.5$, and $1.5$, respectively.
These factors were chosen to balance the impact of each loss function during training.

\begin{table}[h]
    \caption{Official deployed models of YOLOv8}
    \label{tab:yolov8}
    % \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcc}
    \toprule
    \multicolumn{1}{c}{Model}   & \# params (M) & FLOPs (B) \\
    \midrule
    YOLOv8n & 3.2        & 8.7       \\
    YOLOv8s & 11.2       & 28.6      \\
    YOLOv8m & 25.9       & 78.9      \\
    YOLOv8l & 43.7       & 165.2     \\
    YOLOv8x & 68.2       & 257.8     \\
    \bottomrule
    \multicolumn{3}{l}{\textit{Note:} All values in the table correspond}\\
    \multicolumn{3}{l}{\qquad \; to an input image size of 640x640.}
    \end{tabular}%
    % }
\end{table}

\begin{figure}[t]%
    \captionsetup[subfigure]{justification=centering}

    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/2-1.jpg} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/2-2.jpg} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/3-1_zoom.png} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/3-2_zoom.png} }}%
    \hfill
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/1-1.jpg} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/1-2.jpg} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/1-3.jpg} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/1-4.jpg} }}%
    \hfill
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/4-1_zoom.png} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/4-2_zoom.png} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/4-3_zoom.png} }}%
    \subfloat[]{{\includegraphics[height=1.9cm]{fig/img/4-4_zoom.png} }}%

\caption{Qualitative analysis images of driving vehicle and brake light status detection by proposed networks. (a)-(b) is a continuous image pair showcasing the scenario with leading vehicle at a close distance. (c)-(d) is the continuous image pair showcasing the scenario with leading vehicle at a far distance. (e)-(h) are the continuous image pairs showcasing the scenario with multiple vehicles at a close distance. (i)-(l) are the continuous image pairs showcasing the scenario with multiple vehicles at a far distance.
}
\label{fig:qualitative_perf}%
\end{figure}

\begin{figure}[t!]%
    % \captionsetup[subfigure]{justification=centering}

    \subfloat[Highway]{{\includegraphics[height=1.9cm]{fig/Capture/0_highway.jpg} }}%
    \subfloat[City]{{\includegraphics[height=1.9cm]{fig/Capture/0_city.jpg} }}%
    \subfloat[Tunnel]{{\includegraphics[height=1.9cm]{fig/Capture/0_tunnel.jpg} }}%
    \subfloat[Nighttime]{{\includegraphics[height=1.9cm]{fig/Capture/0_night.jpg} }}%
    \hfill
    \subfloat[Motorcycle]{{\includegraphics[height=1.9cm]{fig/Capture/0_motor.jpg} }}%
    \subfloat[Bus]{{\includegraphics[height=1.9cm]{fig/Capture/0_bus.jpg} }}%
    \subfloat[Truck]{{\includegraphics[height=1.9cm]{fig/Capture/0_truck.jpg} }}%
    \subfloat[Special vehicle]{{\includegraphics[height=1.9cm]{fig/Capture/0_special_1.jpg} }}%

\caption{Qualitative analysis images for various driving environments and vehicle types, mainly intended as examples for the following: (a) Hightway, (b) City, (c) Tunnel, (d) Nighttime, (e) Motorcycle, (f) Bus, (g) Truck, and (h) Special vehicle.
}
\label{fig:qualitative_env}%
\end{figure}




\subsection{Results}
\label{sec:exp_results}
In this section, the evaluation results of the trained detection models are presented, both qualitatively and quantitatively.
Qualitative analysis confirmed that the trained detection models accurately detect the bounding boxes of driving vehicles and classify their brake light status (on or off) in various road environments.
Figure \ref{fig:qualitative_perf} shows some of the images used for qualitative analysis.
These images are provided as pairs of two or more consecutive frames to demonstrate clear analysis results.
(a)-(b) represent one continuous image pair displaying the detection performance of leading vehicle located at a close distance.
(c)-(d) represent another continuous image pair displaying the detection performance of leading vehicle located at a far distance.
As evident from these two image pairs, the proposed model accurately detects the location and brake light status of leading vehicles regardless of their distance from the ego vehicle.
(e)-(h) represent other continuous image pairs depicting scenarios with multiple vehicles present at a close distance.
In these images, the location and brake light status of all vehicles in the images are successfully detected.
Furthermore, even in scenarios with multiple vehicles at a far distance, all vehicles are accurately detected, as demonstrated by images pairs (i)-(l).

Qualitative analysis was conducted not only for different vehicle quantities and distances but also for various driving environments and vehicle types.
Figure \ref{fig:qualitative_env} (a)-(d) demonstrate the robust performance of the trained model in diverse driving environments, including highway, city, tunnel, and nighttime.
Figure \ref{fig:qualitative_env} (e)-(h) provide qualitative evidence that the model has ability to detect various vehicle types, including passenger cars, motorcycles, buses, trucks, and special vehicles.

The evaluation of the driving vehicle and brake light status detection performance of each model that underwent transfer learning is conducted by calculating the mean average precision (mAP) on the testset.
Two mAP values are calculated: mAP50 and mAP50-95.
mAP50 represents the average precision at an intersection over union (IoU) threshold of $0.5$. The IoU threshold measures the overlap between the predicted bounding boxes and the ground truth labels, indicating how well the predicted boxes align with the actual objects.
mAP50-95 represents the average precision over a range of IoU thresholds from $0.5$ to $0.95$, with a step size of $0.05$. 
This metric provides comprehensive assessment of the detection model's performance across different levels of overlap.
Both mAP50 and mAP50-95 are commonly used metrics to evaluate the overall performance of object detection models.

\begin{figure}[t]%

    \subfloat[\centering mAP50 on the entire testset]{{\includegraphics[height=5cm]{fig/bar_map50.png} }}%
    \subfloat[\centering mAP50-95 on the entire testset]{{\includegraphics[height=5cm]{fig/bar_map50-95.png} }}%

\caption{Detection performance on the entire testset. The y-axis represents the detection accuracy, and the x-axis lists the YOLOv8 models, with larger models positioned toward the right. Each model is depicted with three bars, showcasing the detection performance for brake light off class, brake light on class, and total class, respectively. Single line plot highlights the difference in detection performance by models for total class.}
\label{fig:test_results}%
\end{figure}

\begin{table}[t!]
    \caption{Results of the entire testset}
    \label{tab:total}
    % \resizebox{\textwidth}{!}{%
    % \begin{tabular}{>{\raggedright}p{2.5cm} >{\raggedright}p{2.5cm} >{\centering}p{2.5cm} >{\centering\arraybackslash}p{2.5cm}}
    \begin{tabular}{llrrrrr}
    \toprule
    \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{Class} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{F1 score} & \multicolumn{1}{c}{mAP50} & \multicolumn{1}{c}{mAP50-95} \\
    \midrule
    \multirow{3}{*}{YOLOv8n}  & Brake Light Off           & 0.726 & 0.670 & 0.697 & 0.761                     & 0.494                          \\
                              & Brake Light On            & 0.549 & 0.853 & 0.668 & 0.772                     & 0.516                        \\
                              & \multicolumn{1}{r}{Total} & 0.638 & 0.762 & 0.695 & 0.766                     & 0.505                        \\
    \midrule
    \multirow{3}{*}{YOLOv8s}  & Brake Light Off           & 0.755 & 0.646 & 0.696 & 0.770                     & 0.502                        \\
                              & Brake Light On            & 0.591 & 0.837 & 0.693 & 0.768                     & 0.511                        \\
                              & \multicolumn{1}{r}{Total} & 0.673 & 0.742 & 0.706 & 0.769                     & 0.507                        \\
    \midrule
    \multirow{3}{*}{YOLOv8m}  & Brake Light Off           & 0.755 & 0.650 & 0.699 & 0.775                     & 0.514                         \\
                              & Brake Light On            & 0.579 & 0.839 & 0.685 & 0.794                     & 0.533                        \\
                              & \multicolumn{1}{r}{Total} & 0.667 & 0.745 & 0.704 & 0.785                     & 0.524                        \\
    \midrule
    \multirow{3}{*}{YOLOv8l}  & Brake Light Off           & 0.746 & 0.655 & 0.698 & 0.775                     & 0.515                         \\
                              & Brake Light On            & 0.552 & 0.867 & 0.675 & 0.799                     & 0.539                        \\
                              & \multicolumn{1}{r}{Total} & 0.649 & 0.761 & 0.701 & 0.787                     & 0.527                      \\
    \midrule
    \multirow{3}{*}{YOLOv8x}  & Brake Light Off           & 0.746 & 0.650 & 0.695 & 0.779                     & 0.518                         \\
                              & Brake Light On            & 0.578 & 0.859 & 0.691 & 0.807                     & 0.546                        \\
                              & \multicolumn{1}{r}{Total} & 0.662 & 0.754 & 0.705 & 0.793                     & 0.532                      \\
    \bottomrule
    \end{tabular}%
    % }
\end{table}

Figure \ref{fig:test_results} presents the detection performance of each trained model, showcasing the results for mAP50 and mAP50-90 in Figure \ref{fig:test_results}-(a) and (b) respectively.
The detection performance for each individual classes, brake light off and brake light on, is represented by blue and red bars respectively. 
The overall detection performance for all classes is shown by the purple bar, with a purple line plot illustrating the trend of performance differences across models.
Both mAP50 and mAP50-95 exhibit similar overall trands, although they differ in scale.
As expected, the detection performance for all classes generally increases as the model size increases.
However, the YOLOv8s model shows a slightly lower performance increase, primarily due to its lower brake light on class detection performance.
Comparing the mAP50 values for each class, it can be observed that all models, except for YOLOv8s, have higher detection performance for the brake light on class compared to the brake light off class.
Overall, the proposed methodology achieved mAP50 values ranging from $0.766$ to $0.793$ and mAP50-95 values ranging from $0.505$ to $0.532$.
Considering the recent benchmarking performance of MS-COCO \cite{lin2014microsoft}, which is one of the leading object detection, with mAP50 values ranging form $71.9$ to $77.0$ and mAP50-95 values ranging from $57.7$ to $58.8$, the proposed methodology demonstrates significant results \cite{coco_benchmark, zou2023object}.
Detailed detection performance for each model and class can be found in Table \ref{tab:total}.

\begin{figure}[b!]%

    \subfloat[mAP50 on the Day testset]{{\includegraphics[height=5cm]{fig/bar_day_map50.png} }}%
    \subfloat[mAP50 on the Night testset]{{\includegraphics[height=5cm]{fig/bar_night_map50.png} }}%
    \hfill
    \subfloat[mAP50-95 on the Day testset]{{\includegraphics[height=5cm]{fig/bar_day_map50-95.png} }}%
    \subfloat[mAP50-95 on the Night testset]{{\includegraphics[height=5cm]{fig/bar_night_map50-95.png} }}%

\caption{Detection performance comparison for ambient illumination difference. The y-axis and x-axis represent the detection accuracy and lists the YOLOv8 models, respectively. Each model is depicted with two bars, showcasing the detection performance for brake light off and brake light on classes. Each line plots highlights the difference in detection performance by models for each class.}
\label{fig:dayNnight}%
\end{figure}

In Figure \ref{fig:test_results}, it was observed that the brake light on detection performance for the brake light on class was generally better than that for the brake light off.
However, since the two classes are distinguished solely based on the brightness of the brake light and not the shape or form of the vehicle, it is possible to hypothesize that ambient illumination can affect the detection performance. 
To verify this hypothesis, the test dataset was split into two types based on ambient light levels: Day, representing images taken during daytime with high ambient illumination, and Night, representing images taken at night or in tunnels with low ambient illumination.
The number of images and annotations for each type are provided in Table \ref{tab:test_dataset}.

\begin{table}[h]
    \caption{Details of test dataset}
    \label{tab:test_dataset}
    % \resizebox{\textwidth*0.5}{!}{%
    \begin{tabular}{p{5cm} p{5cm} p{5cm}}
        % {lrr}
    \toprule
    \multicolumn{1}{c}{Number of}                          & \multicolumn{1}{c}{Day} & \multicolumn{1}{c}{Night} \\
    \midrule
    Images                              & \multicolumn{1}{r}{1,467}                     & \multicolumn{1}{r}{1,729}                    \\
    Annotations                         & \multicolumn{1}{r}{4,911}                    & \multicolumn{1}{r}{5,620}                   \\
    \multicolumn{1}{c}{Brake Light Off ($C_{1}=1.0$)} & \multicolumn{1}{r}{3,563}                    & \multicolumn{1}{r}{2,436}                    \\
    \multicolumn{1}{c}{Brake Light On ($C_{2}=1.0$)}  & \multicolumn{1}{r}{1,348}                     & \multicolumn{1}{r}{3,184}                   \\
    \bottomrule
    \end{tabular}%
    % }
\end{table}



Figure \ref{fig:dayNnight} depicts the detection performance for each class on the Day/Night split test dataset. 
In Figure \ref{fig:dayNnight}-(a) and (b), mAP50 is plotted, while in Figure \ref{fig:dayNnight}-(c) and (d), mAP50-95 is plotted.
Figure \ref{fig:dayNnight}-(a) and (c) show the performance on the Day testset, while (b) and (d) show the performance on the Night testset.
On the Day testset, the detection performance for the brake light off class is better than that for the brake light on class.
Conversely, on the Night testset, the detection performance for the brake light on class is superior to that for the brake light off class.
The brake light off class, which was well detected in an environment with high ambient light, experienced a decline in detection performance as the ambient light decreased.
On the other hand, the brake light on class, which initially exhibited relatively low detection performance in an environment with high ambient light, demonstrated high detection performance when the ambient light was low.
The difference in performance due to ambient illumination is more pronounced in the brake light off class.
Detailed detection performance comparisons for ambient illumination difference for each model and class can be found in Table \ref{tab:dayNnight}.


\begin{table}[!h]
    \caption{Results of comparison for ambient illumination difference}
    \label{tab:dayNnight}
    % \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{0.94}
    \begin{tabular}{llcccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{1}{c}{\multirow{2}{*}{Class}} & \multicolumn{2}{c}{mAP50}                           & \multicolumn{2}{c}{mAP50-95}                        \\
    \multicolumn{1}{c}{}                       & \multicolumn{1}{c}{}                       & \multicolumn{1}{c}{Day} & \multicolumn{1}{c}{Night} & \multicolumn{1}{c}{Day} & \multicolumn{1}{c}{Night} \\
    \midrule
    \multirow{3}{*}{YOLOv8n}                   & Brake Light Off                            & 0.808          & 0.671               & 0.563          & 0.366                     \\
    & Brake Light On                             & 0.699                   & 0.803            & 0.473                   & 0.536            \\
    & \multicolumn{1}{r}{Total}                  & 0.753          & 0.737                     & 0.518          & 0.451                     \\

    \midrule
    \multirow{3}{*}{YOLOv8s}                   & Brake Light Off                            & 0.821          & 0.675                     & 0.574          & 0.369                     \\
    & Brake Light On                             & 0.692                   & 0.804            & 0.466                   & 0.535            \\
    & \multicolumn{1}{r}{Total}                  & 0.757          & 0.739                     & 0.520          & 0.452                     \\

    \midrule
    \multirow{3}{*}{YOLOv8m}                   & Brake Light Off                            & 0.822          & 0.692                     & 0.587          & 0.382                     \\
    & Brake Light On                             & 0.729                   & 0.824            & 0.500                   & 0.550            \\
    & \multicolumn{1}{r}{Total}                  & 0.776                   & 0.758                     & 0.544          & 0.466                     \\

    \midrule
    \multirow{3}{*}{YOLOv8l}                   & Brake Light Off                            & 0.821          & 0.694                     & 0.588          & 0.383                     \\
                                           & Brake Light On                             & 0.733                   & 0.827            & 0.509                   & 0.554            \\
                                           & \multicolumn{1}{r}{Total}                  & 0.777          & 0.761                     & 0.549          & 0.468                    \\
    \midrule
    \multirow{3}{*}{YOLOv8x}                   & Brake Light Off                            & 0.825          & 0.707                     & 0.590          & 0.393                     \\
                                           & Brake Light On                             & 0.740                   & 0.836            & 0.515                   & 0.562            \\
                                           & \multicolumn{1}{r}{Total}                  & 0.782          & 0.772                     & 0.552          & 0.477                    \\
    \bottomrule
    \end{tabular}%
    % }
\end{table}



According to the detailed analysis, the performance difference attributed to the difference in ambient illumination can be explained in terms of accuracy.
The overall accuracies for driving vehicle detection across all classes were $0.87$ in the Day testset and $0.89$ in the Night testset.
As the ambient illumination decreased, the accuracy for driving vehicle detection slightly improved.
However, the accuracies for the brake light off class decreased to $0.67$ in the Day testset and $0.43$ in the Night testset, while the accuracies for the brake light on class increased to $0.75$ in the Day testset and $0.86$ in the Night testset.
The detailed analysis revealed that this performance difference is influenced by the presence of tail lights.
% , the cause of the difference in performance due to the difference in ambient illumination is represented by two factors.
% Therefore, as the ambient illumination decreases, the overall detection performance for driving vehicles improves, while the detection performance for the brake light off class deteriorates and the performance for the brake light on class improves.
% We argue that this phenomenon is influenced by the tail light.
As the ambient illumination decreases, the tail lights of the vehicles are turned on, enhancing the detection performance for driving vehicles.
However, the turned-on tail lights can cause confusion with the turned-on brake lights, leading to a decreases in the detection performance for brake light off class.
Consequently, the decrease in ambient illumination improves the detection performance of vehicles with the brake light turned on while deteriorating the detection performance of vehicles with the brake light turned off.


\begin{figure}[!t]
    \includegraphics[scale=0.5]{fig/plot_inference_time.png}
    \caption{Trade-off between inference time and detection accuracy in different environments. The x-axis represents the inference time and the y-axis represents the detection accuracy as mAP50. It showcases the trends in inference speed and detection performance for differenct sizes of YOLOv8 under different computing environments as follows: ``@GPU'' refers to Nvidia Tesla T4, ``@Nano'' refers to Nvidia Jetson Nano, and ``@CPU'' refers to Intel Xeon processor.}
    \label{fig:inference}
\end{figure}



In order to validate the real-time inference performance of the trained models on edge devices, experiments were conducted to evaluated both accurate detection and inference time. 
The Nvidia Jetson Nano device was utilized for this purpose.
The trained models were converted to the Open Neural Network Exchange (ONNX) format, which is an open format that facilitates the sharing and interoperability of neural network models across different frameworks. The inference time was measured on the Jetson Nano device using the ONNX models.
The measured inference times ranged from $133.30$ ms to $733.27$ ms, depending on the size of the model.
As expected, among the proposed models, YOLOv8n, with the smallest number of parameters and computations, exhibited the fastest inference time of $133.30$ ms, surpassing even human cognitive processing time.
It is worth noting that the average human cognitive response time is approximately 200 ms.
While faster inference time are generally preferred, it is crucial to acknowledge that Jetson Nano operates in a resource-constrained environment.
Despite these strict limitations, YOLOv8n achieved inference time faster than human cognitive processing, indicating that it is sufficiently real-time capability.
The trade-off performance between inference time and detection accuracy is illustrated in Figure \ref{fig:inference}.
To provide a comprehensive performance comparison, the performance on different devices, including the Jetson Nano, CPU (Intel Xeon 2.20GHz), and GPU (Nvidia Tesla T4), was included.
Detailed values can be found in Table \ref{tab:inference}.

\begin{table}[!h]
    \caption{Results of comparison for inference time in different environments. All models were inferred by converting to ONNX form, and the different computing environments are as follows: ``@GPU'' refers to Nvidia Tesla T4, ``@Nano'' refers to Nvidia Jetson Nano, and ``@CPU'' refers to Intel Xeon processor.}
    \label{tab:inference}
    \begin{tabular}{lrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{Model}}   & \multicolumn{3}{c}{Inference Time (ms)} \\
                                 & ONNX @GPU & ONNX @Nano & ONNX @CPU \\
    \midrule
    YOLOv8n & 12.04        & 133.30            & 353.13    \\
    YOLOv8s & 12.16        & 217.20            & 891.93    \\
    YOLOv8m & 20.20        & 471.59            & 1,792.31   \\
    YOLOv8l & 29.09       & 733.27            & 3,340.26   \\
    YOLOv8x & 41.87       & 1,208.69           & 4,222.87  \\
    \bottomrule
    \end{tabular}%
\end{table}


\begin{table}[!h]
    \caption{Comparison of learning-based brake light status detection algorithms.}
    \label{tab:comparison}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcrrclclc}
        \toprule
        \multicolumn{1}{c|}{\multirow{2}{*}{Study}} & \multicolumn{1}{c|}{\multirow{2}{*}{Proposed work}} & \multicolumn{3}{c|}{Dataset}                                                                                                         & \multicolumn{2}{c}{Brake Light Detection}                       & \multicolumn{2}{|c}{Evaluation}                                                                                    \\
        \multicolumn{1}{c|}{}                       & \multicolumn{1}{c|}{}                               & \multicolumn{1}{c}{\# Data}                                        & \multicolumn{1}{c}{\# Samples} & \multicolumn{1}{c|}{Condition $^{1}$} & \multicolumn{1}{c}{Classification} & Localization & \multicolumn{1}{|c}{Performance} & \multicolumn{1}{c}{Inference time (ms)}                                              \\
        \midrule
        Cui et al. \cite{cui2015vision}                                 & HOG, SVM                                           & 10,000 images                                                      & \multicolumn{1}{r}{-- $^{2}$}          & D                             &  Turned On           &              & 0.75 (Detection rate)           & N/T $^{3}$                                                                             \\
        Nava et al. \cite{nava2019collision}                                & YOLO, SVM                                          & 9,700 images                                                       & 9,700                          & D                             &  Turned On/Off       &              & 0.95 (F1 score)                 & N/T $^{3}$                                                                             \\
        Pirhonen et al. \cite{pirhonen2022brake}                            & YOLOv3, RF                                         & 822 images                                                         & 822                            & D                             &  Turned On/Off       &              & 0.82 (Accuracy)                 & N/T $^{3}$                                                                             \\
        Wang et al. \cite{wang2016appearance}                                & HOG, CNN                                           & 5,600 images                                                       & 5,600                          & D                             &  Turned On           &              & 0.89 (Accuracy)                 & N/T $^{3}$                                                                             \\
        Kim \cite{kim2022detecting}                                        & YOLOv4, CNN, LSTM                                  & 189 videos                                                         & \multicolumn{1}{r}{-- $^{2}$}          & T                             &  Turned On/Off       &              & 0.91 (Accuracy)                 & N/T $^{3}$                                                                             \\
        Li et al. \cite{li2020highly}                                 & YOLOv3 tiny (w/SPP)                                & 4,618 images   & 15,197                         & D, N                          &  Turned On           & \checkmark            & 0.89 (mAP50)                    & 15.87 (GTX-1060)                                                                \\
        \midrule
        \textbf{Ours}                                       & YOLOv8n                                            & 11,088 images & 30,444                         & D, N, T                       &  Turned On/Off       & \checkmark            & 0.76 (mAP50)                     & \begin{tabular}[c]{@{}c@{}}12.04 (Tesla T4)\\ 133.30 (Jetson Nano)\end{tabular} \\
        \bottomrule
    \multicolumn{9}{l}{$^{1}$ The abbreviations for condition mean the following respectively: D=Daytime, N=Nighttime, T=Tunnel.}\\
    \multicolumn{9}{l}{$^{2}$ -- = Not reported.}\\
    \multicolumn{9}{l}{$^{3}$ N/T = Not Tested.}
    \end{tabular}
    }
\end{table}

Table \ref{tab:comparison} provides a detailed description of the differences between our proposed model and the key existing brake light status detection studies.
The algorithms listed in Table \ref{tab:comparison} are state-of-the-art learning-based brake light status detection algorithms. 
The five studies listed at the top are divided into two or more stages, involving vehicle localization and classification of brake light status \cite{cui2015vision,nava2019collision,pirhonen2022brake,wang2016appearance,kim2022detecting}.
The methodologies for each stage are sequentially presented under the second column, named proposed work.
It is important to note that these five studies only present evaluation results for brake light status classification, excluding vehicle localization, and hence, the evaluation metrics such as detection rate, F1 score, or accuracy were used to describe the classification performance.
On the other hand, both the algorithms presented by Li et al. \cite{li2020highly} and our proposed algorithm perform vehicle localization and brake light status classification simultaneously in a single-stage process.
The evaluation results provided encompass both vehicle localization and brake light status classification, and mAP50 was used to describe both classification and localization performance.

When comparing the two mAP50 values in the Table \ref{tab:comparison}, Li et al.'s is higher than ours. However, one should be mindful that the number of classification classes and the dataset differ.
The $0.89$ reported by Li et al. \cite{li2020highly} pertains only to the performance considering the turned on brake light status while our $0.76$ accounts for both turned on and off brake light status.
In terms of data and sample size comparison, our research utilized the largest dataset. 
Further more, our dataset includes diverse environmental conditions, including daytime, nighttime, and tunnel scenarios, which were not all simultaneously considered in the other studies.
By integrating this vast amount data of diverse conditions, our experimental results effectively represent a wide range of real-world scenarios.

Among the existing key algorithms in Table \ref{tab:comparison}, the inference time of the algorithms is only presented through the study by Li et al. \cite{li2020highly}, which reported an impressive inference time of $15.87$ ms.
However, it is important to note that their experiments were conducted on a high-performance GPU, GTX-1060, which may have contributed to the rapid inference speed.
In contrast, our study not only utilized powerful GPU but also conducted experiments on edge device with limited resources.
As the experimental environments differed, direct comparison of the inference speeds between the two algorithms is not feasible.
Nonetheless, our study presented the inference time on edge device, showcasing real-time performance. This demonstration yields more practical research results, considering the constraints of edge devices and emphasizing the relevance of our findings for real-world applications.